{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628f3f05-5406-4de1-9267-1840e11840f4",
   "metadata": {},
   "source": [
    "# Fruits and Vegetables Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf2612-a126-4949-8b00-6e8f6f80b1d2",
   "metadata": {},
   "source": [
    "<img src='Fruit-vegetables.jpg' width=1200> "
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe1ba0f6-84d4-4b87-8c62-cbbacb5cd320",
   "metadata": {},
   "source": [
    "Nesne tanima icin Meyve ve Sebze Görüntüleri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5609293-7ee6-4cfc-a405-e8491ebe4c1c",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e54067a-f377-4261-8ed2-132ece815955",
   "metadata": {},
   "source": [
    "This dataset contains images of the following food items:\n",
    "\n",
    "fruits: banana, apple, pear, grapes, orange, kiwi, watermelon, pomegranate, pineapple, mango\n",
    "vegetables: cucumber, carrot, capsicum, onion, potato, lemon, tomato, raddish, beetroot, cabbage, lettuce, spinach, soy bean, cauliflower, bell pepper, chilli pepper, turnip, corn, sweetcorn, sweet potato, paprika, jalepeño, ginger, garlic, peas, eggplant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b2dab-f453-4f5c-bcfa-dc1a0e34e882",
   "metadata": {},
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "raw",
   "id": "668a89af-f031-4580-b883-8a6f1df28235",
   "metadata": {},
   "source": [
    "This dataset contains three folders:\n",
    "\n",
    "train (100 images each)\n",
    "test (10 images each)\n",
    "validation (10 images each) each of the above folders contains subfolders for different fruits and vegetables wherein the images for respective food items are present# Context\n",
    "This dataset contains images of the following food items:\n",
    "\n",
    "fruits: banana, apple, pear, grapes, orange, kiwi, watermelon, pomegranate, pineapple, mango\n",
    "vegetables: cucumber, carrot, capsicum, onion, potato, lemon, tomato, raddish, beetroot, cabbage, lettuce, spinach, soy bean, cauliflower, bell pepper, chilli pepper, turnip, corn, sweetcorn, sweet potato, paprika, jalepeño, ginger, garlic, peas, eggplant"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14e7365d-547f-4b3e-a42a-b036248528ba",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/datasets/kritikseth/fruit-and-vegetable-image-recognition/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce32fc9c-0f05-4e5d-ae1e-4e23d60ee5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D, Dropout, BatchNormalization, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45a39c43-49d4-4178-84a6-908dba69bfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'beetroot', 'bell pepper', 'cabbage', 'capsicum', 'carrot', 'cauliflower', 'chilli pepper', 'corn', 'cucumber', 'eggplant', 'garlic', 'ginger', 'grapes', 'jalepeno', 'kiwi', 'lemon', 'lettuce', 'mango', 'onion', 'orange', 'paprika', 'pear', 'peas', 'pineapple', 'pomegranate', 'potato', 'raddish', 'soy beans', 'spinach', 'sweetcorn', 'sweetpotato', 'tomato', 'turnip', 'watermelon']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Train klasörünün yolu\n",
    "train_path = \"Fruits and Vegetables Image Recognition/train/\"\n",
    "\n",
    "# Klasördeki meyve ve sebze isimlerini al\n",
    "labels = [folder for folder in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, folder))]\n",
    "\n",
    "# İsimleri yazdır\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1145de88-3dcb-4f8f-bdd9-9aed9e4af885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiketler ve resim yolu\n",
    "labels = [\n",
    "    'apple', 'banana', 'beetroot', 'bell pepper', 'cabbage', 'capsicum',\n",
    "    'carrot', 'cauliflower', 'chilli pepper', 'corn', 'cucumber', 'eggplant',\n",
    "    'garlic', 'ginger', 'grapes', 'jalepeno', 'kiwi', 'lemon', 'lettuce',\n",
    "    'mango', 'onion', 'orange', 'paprika', 'pear', 'peas', 'pineapple',\n",
    "    'pomegranate', 'potato', 'raddish', 'soy beans', 'spinach', 'sweetcorn',\n",
    "    'sweetpotato', 'tomato', 'turnip', 'watermelon'\n",
    "]\n",
    "img_path = \"Fruits and Vegetables Image Recognition/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f1a5190-39aa-445d-bbda-06e067d799e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {label: index for index, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "575a47e8-ce2d-4d41-be81-042fad8c9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resim ve etiketleri saklayacak listeler\n",
    "img_list = []\n",
    "label_list = []\n",
    "\n",
    "# Klasörlerdeki resimleri yükleme\n",
    "for label in labels:\n",
    "    folder_path = os.path.join(img_path, label)\n",
    "    if os.path.exists(folder_path):  # Klasörün varlığını kontrol et\n",
    "        for img_file in os.listdir(folder_path):\n",
    "            img_file_path = os.path.join(folder_path, img_file)\n",
    "            img_list.append(img_file_path)\n",
    "            label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d09dc152-283f-4ece-84d6-1188ef643c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'img': img_list, 'label': label_list})\n",
    "df['class'] = df['label'].map(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d00fb305-de83-47cd-a19e-ea7635595e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>apple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>apple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>apple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>apple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>apple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 img  label  class\n",
       "0  Fruits and Vegetables Image Recognition/train/...  apple      0\n",
       "1  Fruits and Vegetables Image Recognition/train/...  apple      0\n",
       "2  Fruits and Vegetables Image Recognition/train/...  apple      0\n",
       "3  Fruits and Vegetables Image Recognition/train/...  apple      0\n",
       "4  Fruits and Vegetables Image Recognition/train/...  apple      0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7da7c68-18c1-4d63-a73a-65684f426706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>watermelon</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>watermelon</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>watermelon</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>watermelon</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>watermelon</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    img       label  class\n",
       "3110  Fruits and Vegetables Image Recognition/train/...  watermelon     35\n",
       "3111  Fruits and Vegetables Image Recognition/train/...  watermelon     35\n",
       "3112  Fruits and Vegetables Image Recognition/train/...  watermelon     35\n",
       "3113  Fruits and Vegetables Image Recognition/train/...  watermelon     35\n",
       "3114  Fruits and Vegetables Image Recognition/train/...  watermelon     35"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c064b8b-50d3-4624-9e31-2943e35cfc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0,\n",
       " 'banana': 1,\n",
       " 'beetroot': 2,\n",
       " 'bell pepper': 3,\n",
       " 'cabbage': 4,\n",
       " 'capsicum': 5,\n",
       " 'carrot': 6,\n",
       " 'cauliflower': 7,\n",
       " 'chilli pepper': 8,\n",
       " 'corn': 9,\n",
       " 'cucumber': 10,\n",
       " 'eggplant': 11,\n",
       " 'garlic': 12,\n",
       " 'ginger': 13,\n",
       " 'grapes': 14,\n",
       " 'jalepeno': 15,\n",
       " 'kiwi': 16,\n",
       " 'lemon': 17,\n",
       " 'lettuce': 18,\n",
       " 'mango': 19,\n",
       " 'onion': 20,\n",
       " 'orange': 21,\n",
       " 'paprika': 22,\n",
       " 'pear': 23,\n",
       " 'peas': 24,\n",
       " 'pineapple': 25,\n",
       " 'pomegranate': 26,\n",
       " 'potato': 27,\n",
       " 'raddish': 28,\n",
       " 'soy beans': 29,\n",
       " 'spinach': 30,\n",
       " 'sweetcorn': 31,\n",
       " 'sweetpotato': 32,\n",
       " 'tomato': 33,\n",
       " 'turnip': 34,\n",
       " 'watermelon': 35}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15de9185-d375-4dc3-a661-138a39fb592c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>apple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>apple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>apple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>apple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>apple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 img  label  class\n",
       "0  Fruits and Vegetables Image Recognition/train/...  apple      0\n",
       "1  Fruits and Vegetables Image Recognition/train/...  apple      0\n",
       "2  Fruits and Vegetables Image Recognition/train/...  apple      0\n",
       "3  Fruits and Vegetables Image Recognition/train/...  apple      0\n",
       "4  Fruits and Vegetables Image Recognition/train/...  apple      0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e58d1b3-bed1-4577-bcf8-767dd3b2b899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>watermelon</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>watermelon</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>watermelon</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>watermelon</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>Fruits and Vegetables Image Recognition/train/...</td>\n",
       "      <td>watermelon</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    img       label  class\n",
       "3110  Fruits and Vegetables Image Recognition/train/...  watermelon     35\n",
       "3111  Fruits and Vegetables Image Recognition/train/...  watermelon     35\n",
       "3112  Fruits and Vegetables Image Recognition/train/...  watermelon     35\n",
       "3113  Fruits and Vegetables Image Recognition/train/...  watermelon     35\n",
       "3114  Fruits and Vegetables Image Recognition/train/...  watermelon     35"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c547287-4f46-4718-9509-ad4cc06a0efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image: Fruits and Vegetables Image Recognition/train/bell pepper\\Image_56.jpg\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for img_file in df['img']:\n",
    "    img = cv2.imread(img_file)\n",
    "    if img is None:  # Resim yüklenemezse hata mesajı ver\n",
    "        print(f\"Error loading image: {img_file}\")\n",
    "        continue\n",
    "    img = cv2.resize(img, (128, 128))  # İstenilen boyutta yeniden boyutlandır\n",
    "    x.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71551c34-166c-407d-92f2-bcf85d7d7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X ve y'yi numpy dizilerine dönüştürme\n",
    "x = np.array(x)\n",
    "y = df['class'].values[:len(x)]  # Sınıf etiketlerini al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6462be9-6896-4367-8e61-c962eaf17acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (3114, 128, 128, 3)\n",
      "y shape: (3114,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Boyutları kontrol et\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Verileri ayır\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "113df4ba-4b9f-463f-8c20-0ece9d2a8fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D, Dropout,BatchNormalization, Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efd225-c824-45df-a4ff-dac8fb347356",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc6371e5-eb88-4748-a281-4c7dbbe1af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Giriş katmanı\n",
    "model.add(Input(shape=(128, 128, 3)))  # Giriş boyutunu güncelledik\n",
    "\n",
    "# Konvolüsyonel katmanlar\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Düzleştirme ve yoğun katmanlar\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))  # Aktivasyon fonksiyonu ekledik\n",
    "model.add(Dense(len(labels), activation='softmax'))  # Çıktı katmanı, sınıf sayısına göre güncellendi\n",
    "\n",
    "# Modeli derleme\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83a5215a-0609-48d0-8a26-9501c31a178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 133ms/step - accuracy: 0.0458 - loss: 74.2624 - val_accuracy: 0.0642 - val_loss: 3.6005\n",
      "Epoch 2/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 138ms/step - accuracy: 0.1955 - loss: 3.1768 - val_accuracy: 0.0722 - val_loss: 3.9304\n",
      "Epoch 3/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.4191 - loss: 2.3039 - val_accuracy: 0.0979 - val_loss: 4.7731\n",
      "Epoch 4/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.6295 - loss: 1.5808 - val_accuracy: 0.1124 - val_loss: 6.1900\n",
      "Epoch 5/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 129ms/step - accuracy: 0.7041 - loss: 1.3466 - val_accuracy: 0.1204 - val_loss: 6.4145\n",
      "Epoch 6/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 168ms/step - accuracy: 0.7768 - loss: 1.0160 - val_accuracy: 0.1220 - val_loss: 6.8845\n",
      "Epoch 7/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 174ms/step - accuracy: 0.8238 - loss: 0.8656 - val_accuracy: 0.1284 - val_loss: 7.9443\n",
      "Epoch 8/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 166ms/step - accuracy: 0.8442 - loss: 0.7414 - val_accuracy: 0.1316 - val_loss: 8.6679\n",
      "Epoch 9/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 161ms/step - accuracy: 0.8442 - loss: 0.7709 - val_accuracy: 0.1252 - val_loss: 10.8604\n",
      "Epoch 10/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 133ms/step - accuracy: 0.8896 - loss: 0.5870 - val_accuracy: 0.1284 - val_loss: 9.8902\n",
      "Epoch 11/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 128ms/step - accuracy: 0.9049 - loss: 0.4968 - val_accuracy: 0.1268 - val_loss: 10.0523\n",
      "Epoch 12/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 153ms/step - accuracy: 0.9053 - loss: 0.5034 - val_accuracy: 0.1300 - val_loss: 11.9403\n",
      "Epoch 13/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 133ms/step - accuracy: 0.9181 - loss: 0.4499 - val_accuracy: 0.1380 - val_loss: 10.5773\n",
      "Epoch 14/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 129ms/step - accuracy: 0.9237 - loss: 0.3788 - val_accuracy: 0.1396 - val_loss: 9.3511\n",
      "Epoch 15/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.9314 - loss: 0.3682 - val_accuracy: 0.1268 - val_loss: 12.9286\n",
      "Epoch 16/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 129ms/step - accuracy: 0.9137 - loss: 0.4408 - val_accuracy: 0.1268 - val_loss: 10.1359\n",
      "Epoch 17/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 130ms/step - accuracy: 0.9398 - loss: 0.3227 - val_accuracy: 0.1188 - val_loss: 12.2918\n",
      "Epoch 18/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 131ms/step - accuracy: 0.9482 - loss: 0.2849 - val_accuracy: 0.1252 - val_loss: 12.0535\n",
      "Epoch 19/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 128ms/step - accuracy: 0.9494 - loss: 0.2772 - val_accuracy: 0.1332 - val_loss: 13.8416\n",
      "Epoch 20/20\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.9486 - loss: 0.3478 - val_accuracy: 0.1268 - val_loss: 12.1377\n"
     ]
    }
   ],
   "source": [
    "# Modeli eğitme\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "942009ff-d14b-4626-b1bf-8f00691d635e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('Fruits_Vegetables_Image_Recognition_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16e583-5d7a-4943-8a35-78b137aa8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.9127 val_accuracy, 20 epoch ile model iyi bir basari gösterdi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b80c5-c1a0-435c-a508-0d9ebb6307ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e859143-5130-48d9-bb88-b7cbc8310cf8",
   "metadata": {},
   "source": [
    "## Transferlearning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ed36df6-2f7b-4048-b89e-ac6d83c4a21f",
   "metadata": {},
   "source": [
    "https://keras.io/api/applications/\n",
    "sayfasindan VGG16 modeliyle tekrar egitiyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d0026bca-bb87-4f85-837e-61052ef3d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13afe52f-f776-47b8-b5f9-1f8971ee73bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2510 images belonging to 36 classes.\n",
      "Found 605 images belonging to 36 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\P53\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 3/79\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:12\u001b[0m 3s/step - accuracy: 0.0469 - loss: -554.2523"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\P53\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 3s/step - accuracy: 0.0247 - loss: -134909.0625 - val_accuracy: 0.0248 - val_loss: -399831.4062\n",
      "Epoch 2/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 3s/step - accuracy: 0.0239 - loss: -969638.5625 - val_accuracy: 0.0248 - val_loss: -1742579.8750\n",
      "Epoch 3/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 3s/step - accuracy: 0.0239 - loss: -2830065.0000 - val_accuracy: 0.0248 - val_loss: -4131840.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 3s/step - accuracy: 0.0239 - loss: -5706071.5000 - val_accuracy: 0.0248 - val_loss: -7552796.5000\n",
      "Epoch 5/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 3s/step - accuracy: 0.0239 - loss: -9601676.0000 - val_accuracy: 0.0248 - val_loss: -11933884.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 3s/step - accuracy: 0.0239 - loss: -14481274.0000 - val_accuracy: 0.0248 - val_loss: -17309618.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 3s/step - accuracy: 0.0239 - loss: -20334158.0000 - val_accuracy: 0.0248 - val_loss: -23674184.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 4s/step - accuracy: 0.0239 - loss: -27096870.0000 - val_accuracy: 0.0248 - val_loss: -30944336.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 4s/step - accuracy: 0.0239 - loss: -34765940.0000 - val_accuracy: 0.0248 - val_loss: -39102760.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 4s/step - accuracy: 0.0239 - loss: -43292548.0000 - val_accuracy: 0.0248 - val_loss: -48109008.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d12c74fb0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir='Fruits and Vegetables Image Recognition/train/'\n",
    "img_width,img_heigth=224,224\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1/255, validation_split=0.20)\n",
    "\n",
    "train_datagenerator=train_datagen.flow_from_directory(directory=data_dir,target_size=(img_width,img_heigth),\n",
    "                                class_mode='binary', subset='training')\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1/255)\n",
    "test_datagenerator=train_datagen.flow_from_directory(directory=data_dir,target_size=(img_width,img_heigth),\n",
    "                                class_mode='binary', subset='validation')\n",
    "\n",
    "base_model=VGG16(weights='imagenet', input_shape=(img_width,img_heigth,3),include_top=False)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(base_model)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_datagenerator,epochs=10,validation_data=test_datagenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54814eb4-7ac5-45ed-b66c-80a0902afcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,691,136</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │      \u001b[38;5;34m25,691,136\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │           \u001b[38;5;34m1,025\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">91,791,173</span> (350.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m91,791,173\u001b[0m (350.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,692,161</span> (98.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,692,161\u001b[0m (98.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,384,324</span> (196.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m51,384,324\u001b[0m (196.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bbe41d16-b3f2-4bc3-afe2-03750bd0af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('Fruits_Vegetables_Image_Recognition_cnn_model_TLv2.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
